{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Setting-up-Environment\" data-toc-modified-id=\"Setting-up-Environment-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setting up Environment</a></span></li><li><span><a href=\"#Importing-Dataset\" data-toc-modified-id=\"Importing-Dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Importing Dataset</a></span></li><li><span><a href=\"#Extrating-Comments-Datasets\" data-toc-modified-id=\"Extrating-Comments-Datasets-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Extrating Comments Datasets</a></span></li><li><span><a href=\"#Data-Pre-processing\" data-toc-modified-id=\"Data-Pre-processing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Pre-processing</a></span></li><li><span><a href=\"#Sentiment-Analysis\" data-toc-modified-id=\"Sentiment-Analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Sentiment Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Calculating-Comment-Scores\" data-toc-modified-id=\"Calculating-Comment-Scores-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Calculating Comment Scores</a></span></li><li><span><a href=\"#Calculating-Post-Scores\" data-toc-modified-id=\"Calculating-Post-Scores-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Calculating Post Scores</a></span></li><li><span><a href=\"#Testing\" data-toc-modified-id=\"Testing-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Testing</a></span></li></ul></li><li><span><a href=\"#Exporting-Dataset\" data-toc-modified-id=\"Exporting-Dataset-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Exporting Dataset</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../Instagram/Cleaned data/\"\n",
    "# filename = \"All_posts_2wikipedia.csv\"\n",
    "filename = \"All_posts_3pareto.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>Username</th>\n",
       "      <th>caption</th>\n",
       "      <th>no. of likes</th>\n",
       "      <th>no. of comments</th>\n",
       "      <th>comments</th>\n",
       "      <th>ID</th>\n",
       "      <th>food</th>\n",
       "      <th>cuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30/6/2020</td>\n",
       "      <td>8days_eat</td>\n",
       "      <td>katongâ€™s famous hokkien mee and fried mee sua ...</td>\n",
       "      <td>326</td>\n",
       "      <td>7</td>\n",
       "      <td>['@lauren.khoury still canâ€™t crack a smile ğŸ˜¬',...</td>\n",
       "      <td>0</td>\n",
       "      <td>hokkien mee</td>\n",
       "      <td>Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30/6/2020</td>\n",
       "      <td>8days_eat</td>\n",
       "      <td>for the first time in singapore, heytea will b...</td>\n",
       "      <td>258</td>\n",
       "      <td>3</td>\n",
       "      <td>['Awesomeeeee neeed this now', '@becauseitsdon...</td>\n",
       "      <td>1</td>\n",
       "      <td>cookies</td>\n",
       "      <td>Snacks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29/6/2020</td>\n",
       "      <td>8days_eat</td>\n",
       "      <td>korean â€œfat-caronsâ€ â€” supersized macarons stuf...</td>\n",
       "      <td>453</td>\n",
       "      <td>3</td>\n",
       "      <td>['#8dayseat #sgfoodies #instafood #yum #sgfood...</td>\n",
       "      <td>2</td>\n",
       "      <td>acar</td>\n",
       "      <td>Malay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29/6/2020</td>\n",
       "      <td>8days_eat</td>\n",
       "      <td>online ordering system oddle has launched a ne...</td>\n",
       "      <td>287</td>\n",
       "      <td>3</td>\n",
       "      <td>['THANK YOU ğŸ‘€ğŸ“¸ğŸ”¥', 'Absolutely love this tinkat...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29/6/2020</td>\n",
       "      <td>8days_eat</td>\n",
       "      <td>a canelÃ© is a bite-sized french pastry from bo...</td>\n",
       "      <td>431</td>\n",
       "      <td>3</td>\n",
       "      <td>['@jet8food , I see it! ğŸ˜', '@brave_nic', 'ğŸ™ğŸ¼ğŸ’—ğŸ’—']</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>31/5/2020</td>\n",
       "      <td>thesilverchef</td>\n",
       "      <td>butadon with egg.\\r\\n\\r\\nwhen you see shabu sh...</td>\n",
       "      <td>1192</td>\n",
       "      <td>18</td>\n",
       "      <td>['Yum yum in my tum ğŸ˜', 'Can you use beef inst...</td>\n",
       "      <td>1035</td>\n",
       "      <td>udon</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>31/5/2020</td>\n",
       "      <td>thesilverchef</td>\n",
       "      <td>hereâ€™s a recap of that famous grilled cheese s...</td>\n",
       "      <td>435</td>\n",
       "      <td>4</td>\n",
       "      <td>['ğŸ™‹ğŸ»\\u200dâ™€ï¸ğŸ™‹ğŸ»\\u200dâ™€ï¸ğŸ™‹ğŸ»\\u200dâ™€ï¸ğŸ™‹ğŸ»\\u200dâ™€ï¸ğŸ™‹ğŸ»\\u...</td>\n",
       "      <td>1036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>31/5/2020</td>\n",
       "      <td>thesilverchef</td>\n",
       "      <td>a healthy sunday start with chobani greek yogh...</td>\n",
       "      <td>353</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>30/5/2020</td>\n",
       "      <td>thesilverchef</td>\n",
       "      <td>crispy salmon in vegetable broth.\\r\\n\\r\\nthank...</td>\n",
       "      <td>461</td>\n",
       "      <td>4</td>\n",
       "      <td>['That skin looksğŸ”¥', 'YummmmğŸ˜‹ğŸ˜‹ğŸ˜‹', 'ğŸ˜˜ğŸ˜˜ğŸ˜˜']</td>\n",
       "      <td>1038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>30/5/2020</td>\n",
       "      <td>thesilverchef</td>\n",
       "      <td>stuffed squid with glutinous rice.\\r\\n\\r\\nsoak...</td>\n",
       "      <td>557</td>\n",
       "      <td>23</td>\n",
       "      <td>['ğŸ˜±ğŸ˜±ğŸ¤¤', 'Really enjoy watching your cooking......</td>\n",
       "      <td>1039</td>\n",
       "      <td>rib</td>\n",
       "      <td>Western</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1040 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      timestamp       Username  \\\n",
       "0     30/6/2020      8days_eat   \n",
       "1     30/6/2020      8days_eat   \n",
       "2     29/6/2020      8days_eat   \n",
       "3     29/6/2020      8days_eat   \n",
       "4     29/6/2020      8days_eat   \n",
       "...         ...            ...   \n",
       "1035  31/5/2020  thesilverchef   \n",
       "1036  31/5/2020  thesilverchef   \n",
       "1037  31/5/2020  thesilverchef   \n",
       "1038  30/5/2020  thesilverchef   \n",
       "1039  30/5/2020  thesilverchef   \n",
       "\n",
       "                                                caption  no. of likes  \\\n",
       "0     katongâ€™s famous hokkien mee and fried mee sua ...           326   \n",
       "1     for the first time in singapore, heytea will b...           258   \n",
       "2     korean â€œfat-caronsâ€ â€” supersized macarons stuf...           453   \n",
       "3     online ordering system oddle has launched a ne...           287   \n",
       "4     a canelÃ© is a bite-sized french pastry from bo...           431   \n",
       "...                                                 ...           ...   \n",
       "1035  butadon with egg.\\r\\n\\r\\nwhen you see shabu sh...          1192   \n",
       "1036  hereâ€™s a recap of that famous grilled cheese s...           435   \n",
       "1037  a healthy sunday start with chobani greek yogh...           353   \n",
       "1038  crispy salmon in vegetable broth.\\r\\n\\r\\nthank...           461   \n",
       "1039  stuffed squid with glutinous rice.\\r\\n\\r\\nsoak...           557   \n",
       "\n",
       "      no. of comments                                           comments  \\\n",
       "0                   7  ['@lauren.khoury still canâ€™t crack a smile ğŸ˜¬',...   \n",
       "1                   3  ['Awesomeeeee neeed this now', '@becauseitsdon...   \n",
       "2                   3  ['#8dayseat #sgfoodies #instafood #yum #sgfood...   \n",
       "3                   3  ['THANK YOU ğŸ‘€ğŸ“¸ğŸ”¥', 'Absolutely love this tinkat...   \n",
       "4                   3  ['@jet8food , I see it! ğŸ˜', '@brave_nic', 'ğŸ™ğŸ¼ğŸ’—ğŸ’—']   \n",
       "...               ...                                                ...   \n",
       "1035               18  ['Yum yum in my tum ğŸ˜', 'Can you use beef inst...   \n",
       "1036                4  ['ğŸ™‹ğŸ»\\u200dâ™€ï¸ğŸ™‹ğŸ»\\u200dâ™€ï¸ğŸ™‹ğŸ»\\u200dâ™€ï¸ğŸ™‹ğŸ»\\u200dâ™€ï¸ğŸ™‹ğŸ»\\u...   \n",
       "1037                0                                                 []   \n",
       "1038                4           ['That skin looksğŸ”¥', 'YummmmğŸ˜‹ğŸ˜‹ğŸ˜‹', 'ğŸ˜˜ğŸ˜˜ğŸ˜˜']   \n",
       "1039               23  ['ğŸ˜±ğŸ˜±ğŸ¤¤', 'Really enjoy watching your cooking......   \n",
       "\n",
       "        ID         food   cuisine  \n",
       "0        0  hokkien mee   Chinese  \n",
       "1        1      cookies    Snacks  \n",
       "2        2         acar     Malay  \n",
       "3        3          NaN       NaN  \n",
       "4        4          NaN       NaN  \n",
       "...    ...          ...       ...  \n",
       "1035  1035         udon  Japanese  \n",
       "1036  1036          NaN       NaN  \n",
       "1037  1037          NaN       NaN  \n",
       "1038  1038          NaN       NaN  \n",
       "1039  1039          rib   Western  \n",
       "\n",
       "[1040 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(filepath + filename)  \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numposts = df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrating Comments Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['@lauren.khoury still canâ€™t crack a smile ğŸ˜¬', '@warren_tqy', '@viggg @bottombracket @riardsg @pamelatlc oh nooooooo!!! Captain Lard!!', '@in_vincentble luckily we went there last week! @albertee01 there goes yr fav CKT!', 'It is my go-to char kway teow bee Hoon spot. At least once a week! I call uncle on his mobile to make sure he is open before I head down because I just canâ€™t stand being disappointed when the craving hits me! ğŸ˜œ been stuck in Oz so Long the craving is unbelievably difficult.']\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"comments\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ğŸ˜±ğŸ˜±ğŸ¤¤',\n",
       " 'Really enjoy watching your cooking....ğŸ§¡',\n",
       " 'Yum! Looks amazing - thanks for the recipe ğŸ™',\n",
       " 'ğŸ˜ Wow!!!',\n",
       " 'Goodness. The liao more than the rice! ğŸ˜‚ğŸ˜‚',\n",
       " 'Kacang la',\n",
       " 'Wahhh super goodğŸ‘ gonna try this recipe!!!',\n",
       " 'Nice',\n",
       " 'ğŸ˜ğŸ˜ğŸ˜',\n",
       " 'Wow yummy',\n",
       " 'Thank you for the recipe!',\n",
       " 'Wahh a lot of work... but indeed worth the hassleğŸ˜‹ğŸ˜‹',\n",
       " 'ğŸ˜˜ğŸ˜˜ğŸ˜˜',\n",
       " '@leeen ğŸ˜']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = df[\"comments\"][1039][2:-2].split('\\', \\'')\n",
    "print(len(comments))\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = pd.DataFrame(columns=[\"ID\", \"comment\"])\n",
    "df[\"no. of scrapped comments\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    comments = df[\"comments\"][i][2:-2].split('\\', \\'')\n",
    "    num_comments = len(comments)\n",
    "    df[\"no. of scrapped comments\"][i] = num_comments\n",
    "    df_comments = df_comments.append(pd.DataFrame({\"ID\": [df[\"ID\"][i]] * num_comments,\n",
    "                                                   \"comment\": comments}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@lauren.khoury still canâ€™t crack a smile ğŸ˜¬</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@warren_tqy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@viggg @bottombracket @riardsg @pamelatlc oh n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@in_vincentble luckily we went there last week...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>It is my go-to char kway teow bee Hoon spot. A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1039</td>\n",
       "      <td>Wow yummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1039</td>\n",
       "      <td>Thank you for the recipe!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1039</td>\n",
       "      <td>Wahh a lot of work... but indeed worth the has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1039</td>\n",
       "      <td>ğŸ˜˜ğŸ˜˜ğŸ˜˜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1039</td>\n",
       "      <td>@leeen ğŸ˜</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6676 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                            comment\n",
       "0      0         @lauren.khoury still canâ€™t crack a smile ğŸ˜¬\n",
       "1      0                                        @warren_tqy\n",
       "2      0  @viggg @bottombracket @riardsg @pamelatlc oh n...\n",
       "3      0  @in_vincentble luckily we went there last week...\n",
       "4      0  It is my go-to char kway teow bee Hoon spot. A...\n",
       "..   ...                                                ...\n",
       "9   1039                                          Wow yummy\n",
       "10  1039                          Thank you for the recipe!\n",
       "11  1039  Wahh a lot of work... but indeed worth the has...\n",
       "12  1039                                                ğŸ˜˜ğŸ˜˜ğŸ˜˜\n",
       "13  1039                                           @leeen ğŸ˜\n",
       "\n",
       "[6676 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "todo: what are the data pre-processing steps required for sentiment analysis? Remove stop words and punctuation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Comment Scores\n",
    "\n",
    "Sentiment scores are give to each comment in each post, ranging from -1 for very negative to +1 for very positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(s):\n",
    "    sia = SIA()\n",
    "    pol_score = sia.polarity_scores(s['comment'])\n",
    "    return s.append(pd.Series(list(pol_score.values()), index=pol_score.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = df_comments.apply(scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments[df_comments[\"ID\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Post Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment score\"] = np.nan\n",
    "df.astype({'sentiment score': 'f'}).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    df[\"sentiment score\"][i] = df_comments[df_comments[\"ID\"] == i][\"compound\"].mean()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentiment score\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View dataset where \"food\" is labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df[\"food\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df[\"food\"].isna()][\"sentiment score\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_single(s):\n",
    "    sia = SIA()\n",
    "    pol_score = sia.polarity_scores(s)\n",
    "    return pol_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for comment in comments:\n",
    "    print(comment)\n",
    "    print(scores_single(comment))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(filepath + filename[:-4] + \"_sentiments\" + \".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
